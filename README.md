# Entropy-to-Zentrube: Redefining Entropy â€” White Papers (v1.8)

This repository hosts the public release of **Zentrube**, a compact, time-aware entropy lens for symbolic drift.  
It reframes entropy as readiness: rising with rupture, falling with recovery, and remaining bounded and interpretable.

## White Papers
- [Brief Version (v1.8) â€” Preview on GitHub](Brief_Zentrube_White%20Paper_v1.8.pdf)  
  [ğŸ“„ Download Brief Version](https://github.com/OMPSHUNYAYA/Entropy-to-Zentrube/raw/main/Brief_Zentrube_White%20Paper_v1.8.pdf)  

- [Detailed Version (v1.8) â€” Preview on GitHub](Zentrube_White%20Paper_v1.8.pdf)  
  [ğŸ“„ Download Detailed Version](https://github.com/OMPSHUNYAYA/Entropy-to-Zentrube/raw/main/Zentrube_White%20Paper_v1.8.pdf)  

## Canonical Formula
Zentrubeâ‚œ = log(Var(xâ‚€:â‚œ) + 1) Ã— exp(âˆ’Î»t)  

- **Logarithmic compression** stabilizes heavy tails.  
- **Exponential decay** introduces a tunable memory horizon (â‰ˆ 1/Î»).  
- As Var â†’ 0 or Î» â†’ âˆ, Zentrubeâ‚œ â†’ 0.

## Scope
- **Observation-only demonstrations** (not predictive, not operational).  
- Reproducible with public datasets across climate, physiology, telecom, finance, cybersecurity, and more.  
- Designed to complement classical entropy by providing a bounded, interpretable view of drift.

## Hero Use Cases
- ğŸŒª **Hurricanes (climate):** 20â€“30% earlier drift signals vs category thresholds.  
- â¤ï¸ **ECG (physiology):** 15â€“25% earlier anomaly visibility with fewer false positives.  
- ğŸ” **Cybersecurity:** earlier DoS onset detection with clear rupture/recovery polarity.  
- ğŸ“ˆ **Insurance:** moderates tail risks by ~20â€“30% through entropy-tempered valuation.  
- ğŸ“¡ **Telecom:** 150â€“200 ms earlier jitter anticipation in join traces.  
- â„ **Snowfall (meteorology):** drift flagged 7â€“14 days before major accumulations.  

(All results are **observation-only, reproducible, falsifiable**, and included in the white papers.)

## Quick Start (Python)

A robust quick-start script with variance, Shannon entropy, and Zentrube:

    from collections import Counter
    import math
    import numpy as np

    def zentrube(x, lam=0.02, S="var"):
        """
        Zentrubeâ‚œ = log(S(xâ‚€:â‚œ) + 1) Ã— exp(âˆ’Î»t)
        - lam: decay rate (1/lam â‰ˆ memory horizon)
        - S: "var" (variance) or "std" (standard deviation)
        """
        arr = np.asarray(x, dtype=float).reshape(-1)
        t = arr.size
        if t == 0:
            return 0.0
        spread = np.var(arr) if S == "var" else np.std(arr)
        return math.log(spread + 1.0) * math.exp(-lam * t)

    def shannon_entropy(x, base="e"):
        """
        Shannon entropy H(X) = âˆ’Î£ p(x) log(p(x))
        base: "e" (nats), "2" (bits), or "10" (bans).
        Treats values as discrete symbols for this quick demo.
        """
        arr = np.asarray(x)
        n = arr.size
        if n == 0:
            return 0.0
        counts = Counter(arr)
        probs = [c/n for c in counts.values()]
        if base == "e":
            log = math.log
        elif base == "2":
            log = lambda v: math.log(v, 2)
        else:
            log = lambda v: math.log(v, 10)
        return -sum(p * log(p) for p in probs if p > 0.0)

    # Example dataset
    x = [1, 2, 3, 4, 5, 6]

    print("Variance:", np.var(x))
    print("Shannon Entropy (nats):", shannon_entropy(x))
    print("Zentrube (lam=0.02):", zentrube(x, lam=0.02))

Typical output:  
- Variance â‰ˆ **2.92** (unbounded, scale-dependent).  
- Shannon Entropy â‰ˆ **1.79** (distribution-centric, no time element).  
- Zentrube â‰ˆ **1.22** (bounded, interpretable, time-aware readiness).

*Tip: Î» controls the memory horizon. A typical range is 0.01â€“0.05; 1/Î» â‰ˆ number of steps remembered.*

## How to read the value (plain English)
- If **Zentrube rises**, the system is **rupturing** (variance growing, instability forming).  
- If **Zentrube falls**, the system is **recovering** (variance stabilizing, order returning).  
- Because of the **exp(âˆ’Î»t)** term, older data counts less â€” the value is always **bounded and time-aware**.

## Quick Comparison (Variance vs Shannon vs Zentrube)

To see why Zentrube matters, compare with classical measures:

Formulas:  
- Variance = Var(xâ‚€:â‚œ)  
- Shannon Entropy = âˆ’Î£ p(x) log(p(x))  
- Zentrubeâ‚œ = log(Var(xâ‚€:â‚œ) + 1) Ã— exp(âˆ’Î»t)

Example (already included in the Quick Start above):

    # Using the same x = [1, 2, 3, 4, 5, 6]
    # Results:
    # - Variance â‰ˆ 2.92
    # - Shannon Entropy â‰ˆ 1.79
    # - Zentrube â‰ˆ 1.22

â¡ï¸ **Impact:** Zentrube gives a compact, early-warning style number thatâ€™s easy to compare across windows, signals, and domains.

## License
Â© The Authors of **Shunyaya Framework** and **Zentrube Formula**.  
Released under **CC BY-NC 4.0** (non-commercial, with attribution).  
Use for research, review, and education. Commercial use and resale prohibited.

## Latest Release
ğŸ‘‰ [Entropy-to-Zentrube: Redefining Entropy â€” White Papers v1.8](https://github.com/OMPSHUNYAYA/Entropy-to-Zentrube/releases/tag/v1.8)

---

## Suggested GitHub Topics (add in repo settings â†’ â€œAboutâ€)
`entropy` Â· `information-theory` Â· `drift-detection` Â· `time-series` Â· `resilience` Â· `zentrube` Â· `shunyaya`
